# 基本常识

## 数据存在哪

!!! danger "请各位用户将所有数据存放至 `/data`  目录下"

目前，服务器的硬盘分为两种：

- 用于存放系统的 1T SSD 
- 用于存放用户数据的 HDD 。

允许用户往系统盘写入曾频繁导致服务器因**系统盘爆满**而宕机，因此，**请各位用户将所有数据（包括 conda 环境）存放至 `/data`  目录下**，防止实验数据不断增多，写满系统盘。

!!! tips "Conda 环境的创建和迁移"
    请参照： [Conda 使用指南](../conda)

(`/data` 目录是基于多块大容量 HDD 的 lvm 分区。)

## 做好宕机的准备

由于各种奇怪的原因，服务器并不能保证永不宕机。因此，请在跑实验时适度保存 checkpoints ，以在意外发生之后能快速恢复到之前的实验状态。

## 内存占用较大可能会被终止

内存爆满也曾多次导致服务器卡死而连接不上。因此，服务器上使用了 [earlyoom - The Early OOM Daemon](https://github.com/rfjakob/earlyoom) 以避免内存爆满。当可用内存低于 10% 后，内存占用最多的进程会被杀死。

!!! tips "保存 checkpoints 以减少实验进程被杀死后的损失"

## 合理使用显存

为了所有用户的体验，请适度调整 batch_size ，并且尽量紧凑地安排显存，占用尽可能少的卡。**不要每张卡占用一半不到的显存但是占用多张卡。**

## 死掉的程序仍可能占用显存

有时，虽然程序在控制台退出了，你已经不再与其交互了，但仍然可能**残存一些子进程占用显存**（常见于多进程多卡训练中，如 PyTorch 的 `DistributedDataParallel` ）。此时通常会产生以下现象：

- 执行 `nvidia-smi` 的输出中，上方统计的显存占用与下方显示的其上各进程占用的显存之和有明显差距。如：（上面是 `20371 MB` ，下面是 `1028+4 MB`）

    ```shell
    +-----------------------------------------------------------------------------+
    | NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |
    |-------------------------------+----------------------+----------------------+
    | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
    | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
    |                               |                      |               MIG M. |
    |===============================+======================+======================|
    |   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |
    | 65%   64C    P8   30W / 350W |  20371MiB / 24268MiB |      3%      Default |
    |                               |                      |                  N/A |
    +-------------------------------+----------------------+----------------------+
    +-----------------------------------------------------------------------------+
    | Processes:                                                                  |
    |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
    |        ID   ID                                                   Usage      |
    |=============================================================================|
    |    0   N/A  N/A      5039      G   /usr/lib/xorg/Xorg                  4MiB |
    |    0   N/A  N/A   1539627      C   python                           1028MiB |
    +-----------------------------------------------------------------------------+
    ```

    !!! warning "在 Docker 中运行程序"
        

        如果在 Docker 中使用 GPU 运行程序， 在宿主机上执行 `nvidia-smi` 看不到进程是正常的。需要在 Docker 中执行  `nvidia-smi` 。其余同理。

- 存在较大显存占用，但是 `GPU-Util` 始终很低（可以使用 `watch -n 0.5 nvidia-smi` 进行持续观察）， `Power` 很低。（但是这两点并不意味着程序一定死了，有可能是程序恰好执行到了 GPU 利用率较低的部分。不过进程号应该能在下方看到。）

这时可以：

!!! tips "1. 通过关键字查找进程"

    使用 `ps aux | grep keyword` 命令查看启动命令（或者 PID 、用户等信息）中包含 `keyword` 的进程。

!!! tips "2. 查看指定显卡设备上的进程"

    使用 `fuser -v /dev/nvidia*` 命令查看所有显卡上的进程号。将 `*` 替换为显卡编号（如 `fuser -v /dev/nvidia1` ）查看指定显卡上的进程号。
    
    注意，此命令只能显示当前用户的进程，需要加 `sudo` 才能显示其它用户的进程。 Docker 中的进程很可能以 root 用户存在，因此也无法直接显示。

巧妙结合上述二者可以显示出指定显卡上进程的详细信息。发现显存异常占用后，如果通过上述方法检查出自己已经退出的程序实际上仍然在占用显存，请及时使用 **`kill -9 <PID>`** 杀死所有残余进程，节省显存空间。
    

