# 注意事项

## 通过 Slurm 系统使用 GPU 资源

参见 [通过 slurm 系统使用 GPU 资源](gpu)

## 磁盘配额

服务器部署了中心化的存储节点`air-storage`，并通过NFS挂载在所有机器的`/home`目录下。

目前，各个实验室在使用存储时均有`3000G`的软配额与`5000G`的硬配额。当存储使用量超过软配额时，有7天的过渡时间将存储使用量下降到配额内；当存储使用量超过软配额7天，或存储使用量超过硬配额，用户将不能再向磁盘写入更多数据。

```shell
# 查看当前用户组的磁盘配额
ssh air-storage quota -gs
```

## 做好宕机等灾难发生的准备

由于各种奇怪的原因，服务器并不能保证永不宕机。为了实验数据以及进度的安全，请做好包括但不限于以下几种准备：

1. 使用 GitHub 等平台云端托管代码，防止硬盘损坏造成代码丢失。
2. 在跑实验时适度保存 checkpoints ，以在宕机等意外发生之后能快速恢复到之前的实验状态。

## 内存占用较大可能会被终止

内存爆满也曾多次导致服务器卡死而连接不上。因此，服务器上使用了 [earlyoom - The Early OOM Daemon](https://github.com/rfjakob/earlyoom) 以避免内存爆满。当可用内存低于 10% 后，内存占用最多的进程会被杀死。

!!! tips "保存 checkpoints 以减少实验进程被杀死后的损失"

## 死掉的程序仍可能占用显存

有时，虽然程序在控制台退出了，你已经不再与其交互了，但仍然可能**残存一些子进程占用显存**（常见于多进程多卡训练中，如 PyTorch 的 `DistributedDataParallel` ）。此时通常会产生以下现象：

- 执行 `nvidia-smi` 的输出中，上方统计的显存占用与下方显示的其上各进程占用的显存之和有明显差距。如：（上面是 `20371 MB` ，下面是 `1028+4 MB`）

    ```shell
    +-----------------------------------------------------------------------------+
    | NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |
    |-------------------------------+----------------------+----------------------+
    | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
    | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
    |                               |                      |               MIG M. |
    |===============================+======================+======================|
    |   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |
    | 65%   64C    P8   30W / 350W |  20371MiB / 24268MiB |      3%      Default |
    |                               |                      |                  N/A |
    +-------------------------------+----------------------+----------------------+
    +-----------------------------------------------------------------------------+
    | Processes:                                                                  |
    |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
    |        ID   ID                                                   Usage      |
    |=============================================================================|
    |    0   N/A  N/A      5039      G   /usr/lib/xorg/Xorg                  4MiB |
    |    0   N/A  N/A   1539627      C   python                           1028MiB |
    +-----------------------------------------------------------------------------+
    ```

    !!! warning "在 Docker 中运行程序"
        

        如果在 Docker 中使用 GPU 运行程序， 在宿主机上执行 `nvidia-smi` 看不到进程是正常的。需要在 Docker 中执行  `nvidia-smi` 。其余同理。

- 存在较大显存占用，但是 `GPU-Util` 始终很低（可以使用 `watch -n 0.5 nvidia-smi` 进行持续观察）， `Power` 很低。（但是这两点并不意味着程序一定死了，有可能是程序恰好执行到了 GPU 利用率较低的部分。不过进程号应该能在下方看到。）

这时可以：

!!! tips "1. 通过关键字查找进程"

    使用 `ps aux | grep keyword` 命令查看启动命令（或者 PID 、用户等信息）中包含 `keyword` 的进程。

!!! tips "2. 查看指定显卡设备上的进程"

    使用 `fuser -v /dev/nvidia*` 命令查看所有显卡上的进程号。将 `*` 替换为显卡编号（如 `fuser -v /dev/nvidia1` ）查看指定显卡上的进程号。
    
    注意，此命令只能显示当前用户的进程，需要加 `sudo` 才能显示其它用户的进程。 Docker 中的进程很可能以 root 用户存在，因此也无法直接显示。

巧妙结合上述二者可以显示出指定显卡上进程的详细信息。发现显存异常占用后，如果通过上述方法检查出自己已经退出的程序实际上仍然在占用显存，请及时使用 **`kill -9 <PID>`** 杀死所有残余进程，节省显存空间。

## 实验环境

1. 所有普通用户没有在集群上通过 `apt` 安装软件包的权限。
2. 一般机器学习所需的 Python 环境可以通过每个用户创建自己的 [conda 环境](conda)来满足；用户有权限在自己的环境下通过 `conda` 或 `pip` 安装所需的 Python 包。
3. NVIDIA GPU 底层驱动以及 CUDA 管理员已经安装好了。遵循一般惯例， CUDA 的安装路径在 `/usr/local` 下，如 `/usr/local/cuda` （默认版本），或者 `/usr/local/cuda11.1` （特定版本）。使用时只需根据这个路径信息配置好环境变量即可。
4. 显然， PyTorch 和其它包一样，在每个实验中的版本都可能不同，因此请在自己的环境中自行安装。在 `/home/share` 下有管理员下载的可以直接 `pip install` 的安装包，如果满足需求可以直接安装至自己的环境使用。

